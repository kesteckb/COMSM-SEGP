## Evaluation

### Evaluation Techniques

#### Timeline of Evaluation

![Evaluation Timeline](evaluation_timeline.png)

We initially considered having another intermediate testing between the wireframe testing and final testing but did not have time to actualize this. However, we were able to perform a focus group test which was not initially planned. 

#### Observations

The type of observation we undertook was direct observation in a controlled environment. For our design (a game-like application played in short sessions), a controlled environment would not be too different from activity in a field. Direct observation was chosen over indirect observation as we found it to be more efficient to note down details during the process rather than capturing the process automatically and analysing for data later. We also did not have the human resources or time to record many tests. This might have limited our data collection in that some details might have been missed by the observer.

This method of evaluation was especially useful near the beginning of our sprints, as the open-ended nature of the participant's interaction with our prototype allowed us to gather information we did not previously consider regarding our design. However, the data collected using this technique is largely qualitative instead of quantitative, so the importance of certain design choices may not be exactly as we interpreted (for example, something that bothered one participant might not be as big of a problem to most of our target users).

#### Interviews

Our observations took place after a brief interview with the participants to find out more about their preferences in gaming. This allowed us to cross reference our findings from the observations with personas and thus create focused user stories.

Unfortunately, for both the interview and the observation, we were unable to interact with our original main target audience (teens and children) because this required an extra ethics approval, which we submitted but was not able to get in time for the project. This meant we only had data on a limited demographic.

For more information about what design choices we made based on the results of the inital round of interviews and observations, please refer to the [UX Design](/UXDesign#user-interviews) section of our report. You can also [click here](user_interview_round_1.pdf) to see our user interview script for the initial round.

#### Focus groups

Our focus group had a format similar to the inital observation and interview, with discussions between the observer and multiple participants at the same time. A focus group allowed us to obtain responses from more participants at once. However, because there were multiple participants, the navigation of the application was controlled by the interviewer, and he would ask the participants to instruct the next steps. Because this evaluation was closer to the end of the build process, we were also more clear of our design intentions and asked more pointed questions about certain design choices we made to get feedback on them. There was sometimes unprompted discussion or brainstorm about certain features of the application, which allowed us to see how our application was interpreted compared to our intentions, much like in the first round of evaluation. Encouraging discussion between the participants also may have resulted in some form of synergy - i.e. some ideas may arise which would not have during one-on-one discussion with just one participant.  

However, within a focus group, the group dynamic may have also affected the discussion negatively, for example, a stronger voice or opinion may overpower everyone else, or people may have been more inclined to voice an opinion in agreement. With a larger group of people, it was also sometimes more difficult to guide the discussion which meant that there were more (perhaps less helpful) tangental discussions. Also, the interviewer noted that the focus group was comprised of his own friends which could have resulted in biased opinion.

#### Questionnaires

We did not use questionnaires for our first evaluation because we wanted to record how the participants interacted with our prototype which would not be possible (or at least not very easy to accomplish) with a questionnaire. However, we recognised that some form of evaluation in the form of questionnaires might have been useful in allowing us to collect more quantitative data (which we currently lack), and may allow us to collect a larger quantity of responses overall. This may also be better suited for a final evaluation to identify how successful we were at achieving our overall goals and objectives.

#### Ethnography

Given that our application was a relatively straightforward game format, we felt that performing an ethnography would not be worth the amount of time and resource that would have needed to go into it. We also believe that none of us are qualified to extract accurate and useful data from this form of evaluation. As such, this was not a method that we considered.

*  Details of how you evaluated your designs (techniques used & awareness of their limitations). Description of  techniques suitable for your particular design. A timeline of evaluation of your design.  
    * Techniques to evaluate
        * Hypotheses driven --> these have been outlined in the design stage
        * What other methods did we use? Check out Interaction Design  5th edition for extra info
    * Limitations of techniques --> These are in the powerpoint for the HCI evaluation lecture. Reference these in a way that shows we understand, not just copy and pasted.
    * Description of techniques suitable to our design
        * Questionnaires
            * evaluate designs and ideas and get demographic info
            * feasible number of questions
            * How is this suitable to our design?
            * What limitations did this have?
        * Observations
            * Observe a user using the product, but record what you see without leading them
            * Are the products used as intended
            * Used throughout development
            * How is this suitable to our design?
            * What limitations did this have?
        * Interviews
            *  one to one interactions with stakeholders and end users
            *  How is this suitable to our design?
            *  What limitations did this have?
        * Focus Groups
            * group interviews to capture feelings and eperiences
            * We'll skip this one.
        * Ethnography
            * shadowing users or method acting and training to become an end user
            * Get a fine-grained understanding of user context (political, social, etc)
            * We can try this, but it might be difficult.
    * Timeline of evaluation
        * How often and when did we do the above methods?
*  Unit testing / Functional testing.
    * Unit testing
        * What methods did we use for Unit testing?
        * This is how we tested different components.
        * Which components did we test?
        * How did we test them?
        * What could have been better and or what was a limitation of the method we used?  
    * Functional testing 
        * What methods did we use?
        * This is a form of black-box texting.
        * What could have been better or what was a limitation of this method?
*  User acceptance testing. Evaluation of your design with users – methods undertaken, findings, implications.
    * User acceptance testing
        * Different from the user interviews, but the final round of user interviews and verifiying the 
    * Evaluation of design with users
        * Methods
            * Can user interviews go here? We can move that from the design section to here if so, and that is already drafted.
            * We can also do observations for this as well
        * Findings
            * Not just findings, but how did this align with our hypotheses?
            * How did we ask questions that were not leading questions?
            * Check out HCI evaluations PowerPoint on this as well. 
        * Implications
            * Did our methods work?
            * Could we expand from sugar to other types of nutritional information?
            * Can we use this method to raise awareness of quantity of sugar in food items?
